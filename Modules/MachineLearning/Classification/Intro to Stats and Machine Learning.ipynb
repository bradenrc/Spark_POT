{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine learning - Classification and Statistics\n",
    "#### Machine Learning - the science of getting computers to act without being explicitly programmed\n",
    "\n",
    "MLlib is Sparkâ€™s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. It consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this example!), dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs.\n",
    "\n",
    "It divides into two packages:\n",
    "- spark.mllib contains the original API built on top of RDDs.\n",
    "- spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n",
    "\n",
    "\n",
    "Using spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supporting spark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming.\n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "<img src='https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/titanic.jpg' width=\"70%\" height=\"70%\"></img>\n",
    "With Spark, we can easily describe data and use it to make predictions.  We'll be using the famous Titanic data set from Kaggle (https://www.kaggle.com/c/titanic/data) and the machine learning package in Spark to do just that.\n",
    "## Access your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-06-28 16:58:23--  https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58625 (57K) [text/plain]\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "--2016-06-28 16:58:23--  https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/train.csv\n",
      "Reusing existing connection to raw.githubusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58625 (57K) [text/plain]\n",
      "Saving to: 'train.csv'\n",
      "\n",
      "100%[======================================>] 58,625      --.-K/s   in 0.004s  \n",
      "\n",
      "2016-06-28 16:58:23 (15.9 MB/s) - 'train.csv' saved [58625/58625]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/train.csv -N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Once we have the data, all of the processing is done in memory.  Here, we're formatting the data, removing columns, dropping rows with insufficient data, creating a DataFrame, and creating columns using user defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>classRank</th>\n",
       "      <th>fare</th>\n",
       "      <th>parChi</th>\n",
       "      <th>sibSpou</th>\n",
       "      <th>survived</th>\n",
       "      <th>cherbourg</th>\n",
       "      <th>queenstown</th>\n",
       "      <th>southampton</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  classRank     fare  parChi  sibSpou  survived  cherbourg  queenstown  \\\n",
       "0   22          3   7.2500       0        1         0          0           0   \n",
       "1   38          1  71.2833       0        1         1          1           0   \n",
       "2   26          3   7.9250       0        0         1          0           0   \n",
       "3   35          1  53.1000       0        1         1          0           0   \n",
       "4   35          3   8.0500       0        0         0          0           0   \n",
       "\n",
       "   southampton  male  female  \n",
       "0            1     1       0  \n",
       "1            0     0       1  \n",
       "2            1     0       1  \n",
       "3            1     0       1  \n",
       "4            1     1       0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext,Row\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "loadTitanicData = sc.textFile(\"train.csv\")\n",
    "header = loadTitanicData.first()\n",
    "loadTitanicData = loadTitanicData.filter(lambda l: l != header).\\\n",
    "                                map(lambda l: l.split(\",\")).\\\n",
    "                                map(lambda l: [l[1],l[2],l[4],l[5],l[6],l[7],l[9],l[11]]).\\\n",
    "                                filter(lambda l: len(l[3]) > 0 and len(l[7]) > 0).\\\n",
    "                                map(lambda l: Row(survived=int(l[0]),\\\n",
    "                                    classRank=int(l[1]),\\\n",
    "                                    sex=l[2],\\\n",
    "                                    age=float(l[3]),\\\n",
    "                                    sibSpou=int(l[4]),\\\n",
    "                                    parChi=int(l[5]),\\\n",
    "                                    fare=float(l[6]),\\\n",
    "                                    embarked=l[7]))\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "titanicDf = sqlContext.createDataFrame(loadTitanicData)\n",
    "\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import IntegerType\n",
    "isCherb = UserDefinedFunction(lambda x: 1 if x == 'C' else 0, IntegerType())\n",
    "isQueen = UserDefinedFunction(lambda x: 1 if x == 'Q' else 0, IntegerType())\n",
    "isSouth = UserDefinedFunction(lambda x: 1 if x == 'S' else 0, IntegerType())\n",
    "isMale = UserDefinedFunction(lambda x: 1 if x == 'male' else 0, IntegerType())\n",
    "isFemale = UserDefinedFunction(lambda x: 1 if x == 'female' else 0, IntegerType())\n",
    "titanicDf = titanicDf.withColumn(\"cherbourg\",isCherb(titanicDf.embarked)).\\\n",
    "                    withColumn(\"queenstown\",isQueen(titanicDf.embarked)).\\\n",
    "                    withColumn(\"southampton\",isSouth(titanicDf.embarked)).\\\n",
    "                    withColumn(\"male\",isMale(titanicDf.sex)).\\\n",
    "                    withColumn(\"female\",isFemale(titanicDf.sex))\n",
    "\n",
    "titanicDf = titanicDf.drop(\"sex\").drop(\"embarked\")\n",
    "titanicDf.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaining insight\n",
    "### Pearson Correlation\n",
    "Now that our data is formatted, we can start to do some basic statistics.  Let's look at what features correlate with surviving the titanic crash.\n",
    "# ADD GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age -0.0824458680434\n",
      "classRank -0.356461588445\n",
      "fare 0.266099600477\n",
      "parChi 0.0952652942869\n",
      "sibSpou -0.0155230236317\n",
      "survived 1.0\n",
      "cherbourg 0.195672717021\n",
      "queenstown -0.0489660937057\n",
      "southampton -0.159015410677\n",
      "male -0.536761623349\n",
      "female 0.536761623349\n"
     ]
    }
   ],
   "source": [
    "for col in titanicDf.columns:\n",
    "    print col + \" \" + str(titanicDf.corr('survived',col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like age had as much impact as we would have guessed.  Let's try to find to correlation after accounting for gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male age -0.119617523233\n",
      "female age 0.110711430946\n"
     ]
    }
   ],
   "source": [
    "maleTitanicDf = titanicDf.filter(titanicDf.male == 1)\n",
    "print \"male age \" + str(maleTitanicDf.corr('survived','age'))\n",
    "femaleTitanicDf = titanicDf.filter(titanicDf.female == 1)\n",
    "print \"female age \" + str(femaleTitanicDf.corr('survived','age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Hypothesis Testing\n",
    "Now that we've seen the correlation, we can double check if they are statistically significant with a chi-squared test:\n",
    "# ADD GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classRank 0.0\n",
      "age 0.102094296376\n",
      "sibSpou 0.000429074888302\n",
      "parChi 6.68106006505e-05\n",
      "fare 4.74847664522e-08\n",
      "cherbourg 1.77768084808e-07\n",
      "queenstown 0.191355954976\n",
      "southampton 2.20492079113e-05\n",
      "male 0.0\n",
      "female 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint \n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "labRDD = titanicDf.map(lambda l: LabeledPoint(l.survived, [l.classRank,l.age,l.sibSpou,l.parChi,l.fare,\\\n",
    "                                                           l.cherbourg,l.queenstown,l.southampton,l.male,l.female]))\n",
    "features = [\"classRank\",\"age\",\"sibSpou\",\"parChi\",\"fare\",\"cherbourg\",\"queenstown\",\"southampton\",\"male\",\"female\"]\n",
    "goodnessOfFitTestResult = Statistics.chiSqTest(labRDD)\n",
    "count = 0\n",
    "for result in goodnessOfFitTestResult:\n",
    "    #print result.pValue\n",
    "    print features[count] + \" \" + str(result.pValue)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Machine Learning\n",
    "We can use observed data to make predictions on guests' survival.  First, we form our data into a usable format, split it into a training set and a test set, and finally, create predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([22.0, 3.0, 7.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]), label=0.0), Row(features=DenseVector([26.0, 3.0, 7.925, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), label=1.0)]\n",
      "\n",
      "[Row(features=DenseVector([38.0, 1.0, 71.2833, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]), label=1.0), Row(features=DenseVector([35.0, 1.0, 53.1, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]), label=1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "titanicDf = titanicDf.map(lambda l: Row(label=float(l.survived),features=\\\n",
    "                                       Vectors.dense([l.age,float(l.classRank),l.fare,float(l.parChi),float(l.sibSpou),\\\n",
    "                                       float(l.cherbourg),float(l.queenstown),float(l.southampton),\\\n",
    "                                       float(l.male),float(l.female)]))).toDF()\n",
    "testDf, trainDf = titanicDf.randomSplit([.15,.85],1)\n",
    "\n",
    "print testDf.take(2)\n",
    "print\n",
    "print trainDf.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model:\n",
    "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "0.171171171171\n",
      "Coefficients:\n",
      "[-0.0158018334818,-0.591343562615,0.00251736383054,-0.026419046642,-0.119469664559,0.29231409134,-0.38845628511,-0.160703792199,-0.900702156103,0.900703796042]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(trainDf)\n",
    "\n",
    "lrPred = lrModel.transform(testDf)\n",
    "print \"Error:\"\n",
    "print lrPred.map(lambda line: (line.label - line.prediction)**2).mean()\n",
    "print \"Coefficients:\"\n",
    "print lrModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model:\n",
    "Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value.\n",
    "<img src='https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Classification/CART_tree_titanic_survivors.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "0.207207207207\n",
      "toDebugString is coming in 2.0, so you can see the entire model.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(trainDf)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(trainDf)\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "dtcPipeline = Pipeline(stages=[labelIndexer, featureIndexer, dtc])\n",
    "\n",
    "dtcModel = dtcPipeline.fit(trainDf)\n",
    "dtcPred = dtcModel.transform(testDf)\n",
    "\n",
    "print \"Error:\"\n",
    "print dtcPred.map(lambda line: (line.label - line.prediction)**2).mean()\n",
    "print \"toDebugString is coming in 2.0, so you can see the entire model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model:\n",
    "Random forests are ensembles of decision trees. They combine many decision trees in order to reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "0.198198198198\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "rfcPipeline = Pipeline(stages=[labelIndexer, featureIndexer, rfc])\n",
    "\n",
    "rfcModel = rfcPipeline.fit(trainDf)\n",
    "rfcPred = rfcModel.transform(testDf)\n",
    "\n",
    "print \"Error:\"\n",
    "print rfcPred.map(lambda line: (line.label - line.prediction)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree:\n",
    "Gradient-Boosted Trees (GBTs) are ensembles of decision trees. GBTs iteratively train decision trees in order to minimize a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "0.234234234234\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "gbtPipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "gbtModel = gbtPipeline.fit(trainDf)\n",
    "gbtPred = gbtModel.transform(testDf)\n",
    "\n",
    "print \"Error:\"\n",
    "print gbtPred.map(lambda line: (line.label - line.prediction)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case, the logistic regression model was the most accurate.  \n",
    "Finally.... the ultimate test... would YOU survive the Titanic crash?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([27.0, 3.0, 50.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]), rawPrediction=DenseVector([1.6247, -1.6247]), probability=DenseVector([0.8354, 0.1646]), prediction=0.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#age,classRank,fare,parChi,sibSpou,cherbourg,queenstown,southampton,male,female\n",
    "userInput = sc.parallelize([Row(features=Vectors.dense([27.0, 3.0,50.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]))]).toDF()\n",
    "lrModel.transform(userInput).take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}