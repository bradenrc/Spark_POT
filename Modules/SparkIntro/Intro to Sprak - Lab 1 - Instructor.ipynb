{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Welcome to Spark!!!\n",
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "This notebook will show you some basic concepts to start working with Apache Spark including:\n",
    "\n",
    "- Understanding Spark Context\n",
    "- Creating Resilient Distributed Datasets (RDD)\n",
    "- Performing Data Transformations\n",
    "- Loading Data Files to use with Spark\n",
    "\n",
    "####Tool Tips:\n",
    "- Notice the navigation and command buttons at top of the notebook. Press Play & Stop buttons to execute code and interupt execution.\n",
    "- Notice each cell has type. (Markdown, Code, Etc) This cell is a Markdown cell which is simply HTML informational vs Code cell allows you to execute against spark.\n",
    "- Notice each cell has desigination, for eample In [n]: the number is cell number. When you see In [*]: that means the cell is executing\n",
    "- To see all methonds available for object you can use Tab key Example Enter \"SC.\" press Tab and a drop down will appear.\n",
    "- To execute code in active cell press play button at top or you can use short cut keys Shift-Enter, Ctrl-Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Spark Driver and Workers programs\n",
    "A Spark program has a driver program and a workers program. Worker programs run on cluster nodes or in local threads. RDDs are distributed\u001d",
    " across workers. \n",
    "\n",
    "<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"80%\" height=\"80%\"></img>\n",
    "\n",
    "###Python Spark (pySpark)\n",
    "We are using the Python programming interface to Spark pySpark. \n",
    "pySpark provides an easy-to-use programming abstraction and parallel runtime.\n",
    "\n",
    "###Spark Context\n",
    "Apache Spark driver application uses a context allow a programming interface to interact with the driver application. This is know as a Spark Context which supports Python, Scala and Java programming languages. The SparkContext object tells Spark how and where to access a cluster.<br>\n",
    "This lab uses IBM's fully managed cloud based notebook enviornment, so the spark context is predefined for you.</font>  Running this notebook in another enviornments might require you to pick an interprerter (i.e. pyspark for python) and create a Spark Config object to initilize a Spark Context. <br>\n",
    "\n",
    "Example:<br>\n",
    "from pyspark import SparkContext, SparkConf<br>\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)<br>\n",
    "sc = SparkContext(conf=conf)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f2351d34990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute Spark Context to see if its active in cluster\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">While its not necessary its a good practice to validate the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Resilient Distributed Datasets\n",
    "\n",
    "Apache Spark uses an abstraction for working with data called RDDs - Resilient Distributed Datasets. An RDD is an immutable fault-tolerant collection of elements that can be operated on in parallel. In Apache Spark all work is expressed by either creating new RDDs, transforming existing RDDs or using RDDs to compute results. When working with RDDs, the Spark Driver application automatically distributes the work accross your cluster.\n",
    "\n",
    "Keep in mind one can construct RDDs a number of ways that include \n",
    "-  Parallelizing existing Python collections (lists) \n",
    "-  Transforming an existing RDDs  \n",
    "-  From files in HDFS or any other storage system. \n",
    "\n",
    "###Understanding Lazy Evaluations...\n",
    "RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Transformations are transformations that do not initiate execution on the cluster. A transformation is mapped in a Digital Acrylic Graph (DAG) which is used to optimize execution on the cluster which occurs at time of an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an RDD from Python collection of numbers\n",
    "\n",
    "#Create a Python collection\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "#Place the collection into an rdd called x_nbr_rdd using parallelize\n",
    "x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Notice no return occurs with sc.parallelize()\n",
    "This means sc.parallelize didn't compute a result, so its a transformation. Spark only recorded how to create the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute an Action and return the first element\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Notice you get a return on .first()\n",
    "This means .first() is an Action. Spark executed all transformations to compute the result of .first()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute a Action and take first 5 elements\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Class', 'My Name is Sparky']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an RDD from Python collection of strings\n",
    "\n",
    "#Create a Python collection\n",
    "y = [\"Hello Class\", \"My Name is Sparky\"]\n",
    "\n",
    "#Place the string value into an rdd called y_str_rdd\n",
    "y_str_rdd = sc.parallelize(y)\n",
    "\n",
    "#Return the first value in yoru RDD - Action\n",
    "y_str_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Transformations\n",
    "As you can see, you created a string \"Hello Human\" and you returned value that was parallelized into RDD first element. If we wanted to work with a corpus of words and run analysis on strings to filter out words, then you would need to map each word into an RDD element.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Some common Transformation Functions\n",
    "\n",
    "- <b>map(func):</b> return a new distributed dataset formed by passing each element of the source through a function func\n",
    "- <b>filter(func):</b> return a new dataset formed by selecting those elements of the source on which func returns true\n",
    "- <b>distinct([numTasks])):</b> return a new dataset that contains the distinct elements of the source dataset\n",
    "- <b>flatMap(func):</b> similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Class. Welcome to the world of Apache Spark.  I love running analysis on data.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD named Words\n",
    "Words = [\"Hello Class. Welcome to the world of Apache Spark.  I love running analysis on data.\"]\n",
    "words_rd = sc.parallelize(Words)\n",
    "words_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Review: Python lambda Functions\n",
    "\n",
    "- Small anonymous functions (not bound to a name) \n",
    "\n",
    "- <b>lambda a , b : a + b</b> returns the sum of its two arguments\n",
    "\n",
    "- Can use lambda functions wherever function objects are required\n",
    "- Restricted to a single expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Class.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Apache',\n",
       " 'Spark.',\n",
       " '',\n",
       " 'I',\n",
       " 'love',\n",
       " 'running',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'data.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\n",
    "Words_rd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Class.', 'Welcome']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform RDD Words into new RDD split on Space character.\n",
    "words_rd2 = words_rd.flatMap(lambda line: line.split(\" \"))\n",
    "words_rd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filter Function\n",
    "The filter command creates a new RDD from another RDD based on a filter criteria.\n",
    "\n",
    "filter syntax is .filter(lambda line: \"Filter Criteria Value\" in line) \n",
    "\n",
    "Hint: Use a simple python print command to add string to your spark results and run multiple actions in single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words Hello Is: 1\n"
     ]
    }
   ],
   "source": [
    "#Count number of \"Hello\" words\n",
    "\n",
    "#Create a new RDD z_str3_rdd for all \"Hello\" words in corpus of words \n",
    "words_rd3 = words_rd2.filter(lambda line: \"Hello\" in line) \n",
    "\n",
    "#Print count of values in the new RDD which represents number of \"Hello\" words in corpus\n",
    "print \"The count of words \" + str(words_rd3.first()+ \" Is: \" + str(words_rd3.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Computations\n",
    "Using Python and Spark functions to perform basic analytics on your data. Let investigate how we can sum a couple elements in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[16] at parallelize at PythonRDD.scala:423\n"
     ]
    }
   ],
   "source": [
    "#Create RDD of array of numbers\n",
    "x = [\"1,2,3,4,5,6,7,8,9,10\"]\n",
    "\n",
    "#Note: Notice the numbers are in \"\" which keeps the values together.\n",
    "#Create an RDD\n",
    "y_rd = sc.parallelize(x)\n",
    "print y_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD of array of numbers\n",
    "x = [\"1,2,3,4,5,6,7,8,9,10\"]\n",
    "\n",
    "#Note: Notice the numbers are in \"\" which keeps the values together.\n",
    "#Create an RDD\n",
    "y_rd = sc.parallelize(x)\n",
    "y_rd.take(1)\n",
    "\n",
    "#Add Values 3 & 10\n",
    "Sum_rd = y_rd.map(lambda y: y.split(\",\")).\\\n",
    "map(lambda y: (int(y[2])+int(y[9])))\n",
    "Sum_rd.take(1)\n",
    "\n",
    "#Note: The trailing \\ is used as a continuation charater ebanabling easier\n",
    "# legalibility of the code \n",
    "#Note: See how we are accessing the second and nineth element in the array\n",
    "## also note the array begins in postion zero versus postion 1\n",
    "#Return Sum Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creating an RDD from a data file\n",
    "\n",
    "Apache Spark can access many data sources (Files, HDFS, APIs, Relational Data Sources, Etc.). These files need to be accessable by your Spark cluster.\n",
    "\n",
    "We will use wget to pull Apache Spark README.md file into your fully managed spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-07-01 10:27:17--  https://github.com/carloapp2/SparkPOT/blob/master/README.md\n",
      "Resolving github.com (github.com)... 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: 'README.md'\n",
      "\n",
      "    [ <=>                                   ] 39,412      --.-K/s   in 0.07s   \n",
      "\n",
      "2016-07-01 10:27:18 (568 KB/s) - 'README.md' saved [39412]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm README.md* -f README.md\n",
    "!wget https://github.com/carloapp2/SparkPOT/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use SparkContext textFile to convert a text file to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put Data file into RDD\n",
    "textfile_rdd = sc.textFile(\"README.md\")\n",
    "textfile_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[25] at RDD at PythonRDD.scala:43\n",
      "The file README.md has the word SPARK in it 49 Times.\n"
     ]
    }
   ],
   "source": [
    "#Create a new RDD for all words \"Spark\" in text file\n",
    "Spark_rdd = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "print Spark_rdd\n",
    "#Print the count of elements in new RDD\n",
    "print \"The file README.md has the word SPARK in it \" + str(Spark_rdd.count()) + ' Times.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}